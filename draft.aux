\relax 
\citation{SchultzW:1993:Dopamine}
\citation{MontaguePR:1996:Dopamine,SchultzW:1997:Science}
\citation{DayanP:2008:Ugly}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{sec:introduction}{{1}{1}}
\citation{SuttonRS:1998:Book}
\citation{TesauroG:1995:TDGammon}
\citation{DayanP:1992:Proof}
\citation{Boyan:1995:Approximating}
\citation{RumelhartDE:1986:BP}
\citation{TesauroG:1995:TDGammon}
\citation{Boyan:1995:Approximating}
\citation{SuttonRS:1998:Book,Botvinick-Niv-Barto:2009:HRL}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{3}}
\newlabel{sec:background}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Temporal Difference Learning}{3}}
\newlabel{sub:temporal_difference_learning}{{2.1}{3}}
\citation{Boyan:1995:Approximating}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig-sub:actor-critic-schematic}{{1a}{4}}
\newlabel{sub@fig-sub:actor-critic-schematic}{{a}{4}}
\newlabel{fig-sub:actor-critic-brain-schematic}{{1b}{4}}
\newlabel{sub@fig-sub:actor-critic-brain-schematic}{{b}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An actor-critic RL. (a) Schematic of the RL actorâ€“critic model structure. $r$: reward signal; $Q(s,a)$: state-action value function; $\delta $: temporal- difference prediction error; $\pi (s)$: policy function. (b) An actor-critic model of brain. \emph  {VS}: ventral Striatum, \emph  {DLS}: dorso-lateral Striatum, \emph  {DA}: dopamine, \emph  {HT+}: Hypothalamus.\relax }}{4}}
\newlabel{fig:TD-schematic}{{1}{4}}
\citation{SuttonRS:1996:Coarse}
\citation{AlbusJS:1975:CMAC}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The agent receives $-1$ reward on each time step until it reaches the goal in the Northeast corner. The agent moves a distance of $0.05$ either North, South, East, or West on each time step. Entering a puddle produces a reward of $(-400 \times d)$, where $d$ is the shortest distance to a puddle edge.\relax }}{5}}
\newlabel{puddleworld}{{2}{5}}
\citation{SuttonRS:1996:Coarse}
\citation{SuttonRS:1996:Coarse}
\citation{OReillyRC:2001:CECN}
\citation{KandelE:2012:Book}
\citation{OReillyRC:2001:CECN}
\citation{OReillyRC:2001:CECN}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental Design}{7}}
\newlabel{sec:experimental_design}{{3}{7}}
\citation{SuttonRS:1998:Book}
\@writefile{toc}{\contentsline {section}{\numberline {4}Simulation Tasks}{8}}
\newlabel{sec:simulation_tasks}{{4}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Puddle World}{8}}
\newlabel{sub:puddle_world}{{4.1}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces All networks had the same input layer and output layer, with $42$ input units and $4$ output units for the puddle world task, as an example. The linear network had no hidden layer, while the standard backpropagation network and the kWTA network had $220$ hidden units. The kWTA network produced sparse codes by allowing only $22$ hidden units to have non-zero activation at any one time.\relax }}{9}}
\newlabel{fig:networks}{{3}{9}}
\citation{SuttonRS:1996:Coarse}
\citation{RumelhartDE:1986:BP}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The goal is to drive an underpowered car above the steep hill. The agent received -1 reward for each step until it reaches to goal which received 0 reward.\relax }}{11}}
\newlabel{fig:mountain_car}{{4}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Mountain Car}{11}}
\newlabel{sub:mountain_car}{{4.2}{11}}
\citation{RumelhartDE:1986:BP}
\citation{SuttonRS:1996:Coarse}
\citation{SuttonRS:1996:Coarse}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Acrobot Control Task}{13}}
\newlabel{sub:acrobot_control_task}{{4.3}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The goal is swinging until to reach the tip (``feet'') above the bar by an amount of length of a the ``hand'' link. The agent receives -1 reward until it reaches to goal which received 0 reward.\relax }}{14}}
\newlabel{fig:acrobot}{{5}{14}}
\citation{RumelhartDE:1986:BP}
\bibstyle{elsarticle-harv}
\bibdata{BICA}
\bibcite{AlbusJS:1975:CMAC}{{1}{1975}{{Albus}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulation Results}{16}}
\newlabel{sec:simulation_results}{{5}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Puddle World Task}{16}}
\newlabel{sub:puddle_world_results}{{5.1}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Mountain Car Task}{16}}
\newlabel{sub:mountain_car_results}{{5.2}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Acrobot Task}{16}}
\newlabel{sub:acrobot_results}{{5.3}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {Appendix A}acrobot}{16}}
\newlabel{appen:acrobot}{{Appendix A}{16}}
\bibcite{Botvinick-Niv-Barto:2009:HRL}{{2}{2009}{{Botvinick~MM and AC.}}{{}}}
\bibcite{Boyan:1995:Approximating}{{3}{1995}{{Boyan and Moore}}{{}}}
\bibcite{DayanP:1992:Proof}{{4}{1992}{{Dayan}}{{}}}
\bibcite{DayanP:2008:Ugly}{{5}{2008}{{Dayan and Niv}}{{}}}
\bibcite{KandelE:2012:Book}{{6}{2012}{{Kandel et~al.}}{{Kandel, Schwartz, Jessell, Siegelbaum, and Hudspeth}}}
\bibcite{MontaguePR:1996:Dopamine}{{7}{1996}{{Montague et~al.}}{{Montague, Dayan, and Sejnowski}}}
\bibcite{OReillyRC:2001:CECN}{{8}{2001}{{O'Reilly and Munakata}}{{}}}
\bibcite{RumelhartDE:1986:BP}{{9}{1986}{{Rumelhart et~al.}}{{Rumelhart, Hinton, and Williams}}}
\bibcite{SchultzW:1993:Dopamine}{{10}{1993}{{Schultz et~al.}}{{Schultz, Apicella, and Ljungberg}}}
\bibcite{SchultzW:1997:Science}{{11}{1997}{{Schultz et~al.}}{{Schultz, Dayan, and Montague}}}
\bibcite{SuttonRS:1996:Coarse}{{12}{1996}{{Sutton}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {Appendix B}algorithm}{17}}
\newlabel{appen:algorithm}{{Appendix B}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {Appendix C}mountain car}{17}}
\newlabel{appen:mountain-car}{{Appendix C}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {Appendix D}puddle world}{17}}
\newlabel{appen:puddle-world}{{Appendix D}{17}}
\bibcite{SuttonRS:1998:Book}{{13}{1998}{{Sutton and Barto}}{{}}}
\bibcite{TesauroG:1995:TDGammon}{{14}{1995}{{Tesauro}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The performance of various learned value function approximators may be compared in terms of their success at learning the true value function, the resulting action selection policy, and the amount of experience in the environment needed to learn. The approximated value functions, expressed as $\unhbox \voidb@x \hbox {\it  max}_{a}\ \mathaccentV {hat}05E{Q}(s,a)$ for each location, $s$, appears on the left. The actions selected at a grid of locations is shown in the middle column. The learning curve, showing the TD Error over learning episodes, is shown on the right. The rows display results for representative neural networks of the three kinds explored: linear, backpropagation, and kWTA, respectively, from top to bottom.\relax }}{19}}
\newlabel{bigresults}{{6}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The performance of various networks trained for mountain car.\relax }}{20}}
\newlabel{bigresults}{{7}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The \relax }}{21}}
\newlabel{bigresults}{{8}{21}}
